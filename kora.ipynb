{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if cap.isOpened() is False:\n",
    "        raise(\"IO Error\")\n",
    "\n",
    "    cv2.namedWindow(\"Capture\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            continue\n",
    "\n",
    "        cv2.imshow(\"Capture\", image)\n",
    "       \n",
    "        if cv2.waitKey(33) >= 0:\n",
    "            cv2.imwrite(\"image.png\", image)\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    cv2.imshow(\"image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.07027931687\n",
      "20.2610695232\n",
      "5.81149853621\n",
      "ここで正面左右選択してくれえ\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "msuo tvをロードしました.\n",
      "Number of faces detected: 1\n",
      "とおったよおおおおおおおおおおおおおおお\n",
      "ここでみれるぞおおおおおおおおおおおおおおお\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#一人で映ろうね！！！！！！！！！！！！！！！！！！！\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy \n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "PREDICTOR_PATH = './shape_predictor_68_face_landmarks.dat'\n",
    "PREDICTOR = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "\n",
    "#顔の向き初期設定↓！！！！！！！！！！\n",
    "K = [6.5308391993466671e+002, 0.0, 3.1950000000000000e+002,\n",
    "     0.0, 6.5308391993466671e+002, 2.3950000000000000e+002,\n",
    "     0.0, 0.0, 1.0]\n",
    "D = [7.0834633684407095e-002, 6.9140193737175351e-002, 0.0, 0.0, -1.3073460323689292e+000]\n",
    "\n",
    "cam_matrix = np.array(K).reshape(3, 3).astype(np.float32)\n",
    "dist_coeffs = np.array(D).reshape(5, 1).astype(np.float32)\n",
    "\n",
    "object_pts = np.float32([[6.825897, 6.760612, 4.402142],\n",
    "                         [1.330353, 7.122144, 6.903745],\n",
    "                         [-1.330353, 7.122144, 6.903745],\n",
    "                         [-6.825897, 6.760612, 4.402142],\n",
    "                         [5.311432, 5.485328, 3.987654],\n",
    "                         [1.789930, 5.393625, 4.413414],\n",
    "                         [-1.789930, 5.393625, 4.413414],\n",
    "                         [-5.311432, 5.485328, 3.987654],\n",
    "                         [2.005628, 1.409845, 6.165652],\n",
    "                         [-2.005628, 1.409845, 6.165652],\n",
    "                         [2.774015, -2.080775, 5.048531],\n",
    "                         [-2.774015, -2.080775, 5.048531],\n",
    "                         [0.000000, -3.116408, 6.097667],\n",
    "                         [0.000000, -7.415691, 4.070434]])\n",
    "\n",
    "reprojectsrc = np.float32([[10.0, 10.0, 10.0],\n",
    "                           [10.0, 10.0, -10.0],\n",
    "                           [10.0, -10.0, -10.0],\n",
    "                           [10.0, -10.0, 10.0],\n",
    "                           [-10.0, 10.0, 10.0],\n",
    "                           [-10.0, 10.0, -10.0],\n",
    "                           [-10.0, -10.0, -10.0],\n",
    "                           [-10.0, -10.0, 10.0]])\n",
    "\n",
    "line_pairs = [[0, 1], [1, 2], [2, 3], [3, 0],\n",
    "              [4, 5], [5, 6], [6, 7], [7, 4],\n",
    "              [0, 4], [1, 5], [2, 6], [3, 7]]\n",
    "\n",
    "#顔の向き初期設定終わり↑！！！！！！！！！！\n",
    "\n",
    "class NoFaces(Exception):\n",
    "    pass\n",
    "\n",
    "class Face:\n",
    "  def __init__(self, image, rect):\n",
    "    self.image = image\n",
    "    self.landmarks = numpy.matrix(\n",
    "      [[p.x, p.y] for p in PREDICTOR(image, rect).parts()]\n",
    "    )\n",
    "\n",
    "class BeBean:\n",
    "  SCALE_FACTOR = 1\n",
    "  FEATHER_AMOUNT = 11\n",
    "\n",
    "  # 特徴点のうちそれぞれの部位を表している配列のインデックス\n",
    "  FACE_POINTS = list(range(17, 68))\n",
    "  MOUTH_POINTS = list(range(48, 61))\n",
    "  RIGHT_BROW_POINTS = list(range(17, 22))\n",
    "  LEFT_BROW_POINTS = list(range(22, 27))\n",
    "  RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "  LEFT_EYE_POINTS = list(range(42, 48))\n",
    "  NOSE_POINTS = list(range(27, 35))\n",
    "  JAW_POINTS = list(range(0, 17))\n",
    "\n",
    "  ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS + RIGHT_BROW_POINTS +\n",
    "    NOSE_POINTS + MOUTH_POINTS)\n",
    "\n",
    "  # オーバーレイする特徴点\n",
    "  OVERLAY_POINTS = [LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
    "    NOSE_POINTS + MOUTH_POINTS]\n",
    "\n",
    "  COLOR_CORRECT_BLUR_FRAC = 0.7\n",
    "\n",
    "  def __init__(self, image_path, before_after = True):\n",
    "    self.detector = dlib.get_frontal_face_detector()\n",
    "    self._load_beans(image_path)\n",
    "    self.before_after = before_after\n",
    "\n",
    "  def load_faces_from_image(self, image_path):\n",
    "    \"\"\"\n",
    "      画像パスから画像オブジェクトとその画像から抽出した特徴点を読み込む。\n",
    "      ※ 画像内に顔が1つないし複数検出された場合も、返すので正確には「特徴点配列」の配列を返す\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (image.shape[1] * self.SCALE_FACTOR,\n",
    "                               image.shape[0] * self.SCALE_FACTOR))\n",
    "    \n",
    "    rects = self.detector(image, 1)\n",
    "\n",
    "    if len(rects) == 0:\n",
    "      raise NoFaces\n",
    "    else:\n",
    "      print(\"Number of faces detected: {}\".format(len(rects)))\n",
    "    faces = [Face(image, rect) for rect in rects]\n",
    "    return image, faces\n",
    "\n",
    "  def transformation_from_points(self, t_points, o_points):\n",
    "    \"\"\"\n",
    "      特徴点から回転やスケールを調整する。\n",
    "      t_points: (target points) 対象の特徴点(入力画像)\n",
    "      o_points: (origin points) 合成元の特徴点(つまりビーン)\n",
    "    \"\"\"\n",
    "\n",
    "    t_points = t_points.astype(numpy.float64)\n",
    "    o_points = o_points.astype(numpy.float64)\n",
    "\n",
    "    t_mean = numpy.mean(t_points, axis = 0)\n",
    "    o_mean = numpy.mean(o_points, axis = 0)\n",
    "\n",
    "    t_points -= t_mean\n",
    "    o_points -= o_mean\n",
    "\n",
    "    t_std = numpy.std(t_points)\n",
    "    o_std = numpy.std(o_points)\n",
    "\n",
    "    t_points -= t_std\n",
    "    o_points -= o_std\n",
    "\n",
    "    # 行列を特異分解しているらしい\n",
    "    # https://qiita.com/kyoro1/items/4df11e933e737703d549\n",
    "    U, S, Vt = numpy.linalg.svd(t_points.T * o_points)\n",
    "    R = (U * Vt).T\n",
    "\n",
    "    return numpy.vstack(\n",
    "      [numpy.hstack((( o_std / t_std ) * R, o_mean.T - ( o_std / t_std ) * R * t_mean.T )),\n",
    "      numpy.matrix([ 0., 0., 1. ])]\n",
    "    )\n",
    "\n",
    "  def get_face_mask(self, face):\n",
    "    image = numpy.zeros(face.image.shape[:2], dtype = numpy.float64)\n",
    "    for group in self.OVERLAY_POINTS:\n",
    "      self._draw_convex_hull(image, face.landmarks[group], color = 1)\n",
    "\n",
    "    image = numpy.array([ image, image, image ]).transpose((1, 2, 0))\n",
    "    image = (cv2.GaussianBlur(image, (self.FEATHER_AMOUNT, self.FEATHER_AMOUNT), 0) > 0) * 1.0\n",
    "    image = cv2.GaussianBlur(image, (self.FEATHER_AMOUNT, self.FEATHER_AMOUNT), 0)\n",
    "\n",
    "    return image\n",
    "\n",
    "  def warp_image(self, image, M, dshape):\n",
    "    output_image = numpy.zeros(dshape, dtype = image.dtype)\n",
    "    cv2.warpAffine(\n",
    "      image,\n",
    "      M[:2],\n",
    "      (dshape[1], dshape[0]),\n",
    "      dst = output_image, borderMode = cv2.BORDER_TRANSPARENT, flags = cv2.WARP_INVERSE_MAP\n",
    "    )\n",
    "    return output_image\n",
    "\n",
    "  def correct_colors(self, t_image, o_image, t_landmarks):\n",
    "    \"\"\"\n",
    "      対象の画像に合わせて、色を補正する\n",
    "    \"\"\"\n",
    "    blur_amount = self.COLOR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n",
    "      numpy.mean(t_landmarks[self.LEFT_EYE_POINTS], axis = 0) -\n",
    "      numpy.mean(t_landmarks[self.RIGHT_EYE_POINTS], axis = 0)\n",
    "    )\n",
    "    blur_amount = int(blur_amount)\n",
    "\n",
    "    if blur_amount % 2 == 0: blur_amount += 1\n",
    "\n",
    "    t_blur = cv2.GaussianBlur(t_image, (blur_amount, blur_amount), 0)\n",
    "    o_blur = cv2.GaussianBlur(o_image, (blur_amount, blur_amount), 0)\n",
    "\n",
    "    # ゼロ除算を避ける　\n",
    "    o_blur += (128 * (o_blur <= 1.0)).astype(o_blur.dtype)\n",
    "\n",
    "    return (o_image.astype(numpy.float64) * t_blur.astype(numpy.float64) / o_blur.astype(numpy.float64))\n",
    "\n",
    "  def to_bean(self, image_path):\n",
    "    original, faces = self.load_faces_from_image(image_path)\n",
    "\n",
    "    # base_imageに合成していく\n",
    "    base_image = original.copy()\n",
    "\n",
    "    for face in faces:\n",
    "      bean = self._get_bean_similar_to(face)\n",
    "      bean_mask = self.get_face_mask(bean)\n",
    "\n",
    "      M = self.transformation_from_points(\n",
    "        face.landmarks[self.ALIGN_POINTS],\n",
    "        bean.landmarks[self.ALIGN_POINTS]\n",
    "      )\n",
    "\n",
    "      warped_bean_mask = self.warp_image(bean_mask, M, base_image.shape)\n",
    "      combined_mask = numpy.max(\n",
    "        [self.get_face_mask(face), warped_bean_mask], axis = 0\n",
    "      )\n",
    "\n",
    "      warped_image = self.warp_image(bean.image, M, base_image.shape)\n",
    "      print ('ここでみれるぞおおおおおおおおおおおおおおお')\n",
    "      cv2.imshow(\"image1\",bean.image)#ここやぞおおおおおおおおおおおおおおおおおおおおおおおおおおおおおお\n",
    "      warped_corrected_image = self.correct_colors(base_image, warped_image, face.landmarks)\n",
    "      base_image = base_image * (1.0 - combined_mask) + warped_corrected_image * combined_mask\n",
    "\n",
    "    path, ext = os.path.splitext( os.path.basename(image_path) )\n",
    "    cv2.imwrite('outputs/output_' + path + ext, base_image)\n",
    "\n",
    "    if self.before_after is True:\n",
    "      before_after = numpy.concatenate((original, base_image), axis = 1)\n",
    "      cv2.imwrite('before_after/' + path + ext, before_after)\n",
    "\n",
    "  def _draw_convex_hull(self, image, points, color):\n",
    "    \"指定したイメージの領域を塗りつぶす\"\n",
    "\n",
    "    points = cv2.convexHull(points)\n",
    "    cv2.fillConvexPoly(image, points, color = color)\n",
    "  #顔の向きのやつ↓\n",
    "  def get_head_pose(self,shape):\n",
    "    image_pts = np.float32([shape[17], shape[21], shape[22], shape[26], shape[36],\n",
    "                            shape[39], shape[42], shape[45], shape[31], shape[35],\n",
    "                            shape[48], shape[54], shape[57], shape[8]])\n",
    "\n",
    "    _, rotation_vec, translation_vec = cv2.solvePnP(object_pts, image_pts, cam_matrix, dist_coeffs)\n",
    "\n",
    "    reprojectdst, _ = cv2.projectPoints(reprojectsrc, rotation_vec, translation_vec, cam_matrix,\n",
    "                                        dist_coeffs)\n",
    "\n",
    "    reprojectdst = tuple(map(tuple, reprojectdst.reshape(8, 2)))\n",
    "\n",
    "    # calc euler angle\n",
    "    rotation_mat, _ = cv2.Rodrigues(rotation_vec)\n",
    "    pose_mat = cv2.hconcat((rotation_mat, translation_vec))\n",
    "    _, _, _, _, _, _, euler_angle = cv2.decomposeProjectionMatrix(pose_mat)\n",
    "\n",
    "    return reprojectdst, euler_angle\n",
    "\n",
    "  def head_pose_estimation(self,image_path):\n",
    "    \n",
    "    frame = cv2.imread(image_path)\n",
    "    face_rects = detector(frame, 0)\n",
    "\n",
    "    if len(face_rects) > 0:\n",
    "        shape = PREDICTOR(frame, face_rects[0])\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        reprojectdst, euler_angle = self.get_head_pose(shape)\n",
    "                \n",
    "        #Esc,enterで脱出\n",
    "        print(euler_angle[0, 0])\n",
    "        print(euler_angle[1, 0])\n",
    "        print(euler_angle[2, 0])\n",
    "    \n",
    "  #顔の向き終わり↑\n",
    "  def _load_beans(self,image_path):\n",
    "    \"Mr. ビーンの画像をロードして、顔(特徴点など)を検出しておく\"\n",
    "    \n",
    "    self.head_pose_estimation(image_path)\n",
    "    \n",
    "    self.beans = []\n",
    "    print(\"ここで正面左右選択してくれえ\")\n",
    "    #if(hogehoge)\n",
    "    for image_path in glob.glob(os.path.join('all', '*.jpg')):\n",
    "      image, bean_face = self.load_faces_from_image(image_path)\n",
    "      self.beans.append(bean_face[0])  \n",
    "    print('msuo tvをロードしました.')\n",
    "\n",
    "  def _get_bean_similar_to(self, face):\n",
    "    \"特徴点の差分距離が小さいMr.ビーンを返す\"\n",
    "\n",
    "    get_distances = numpy.vectorize(lambda bean: numpy.linalg.norm(face.landmarks - bean.landmarks))\n",
    "\n",
    "    distances = get_distances(self.beans)\n",
    "    print('とおったよおおおおおおおおおおおおおおお')\n",
    "    return self.beans[distances.argmin()]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    if capture.isOpened() is False:\n",
    "        raise(\"IO Error\")\n",
    "\n",
    "    cv2.namedWindow(\"Capture\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret, image = capture.read()\n",
    "\n",
    "        if ret == False:\n",
    "            continue\n",
    "\n",
    "        cv2.imshow(\"Capture\", image)\n",
    "       \n",
    "        if cv2.waitKey(33) >= 0:\n",
    "            cv2.imwrite(\"image.jpg\", image)\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    frame = cv2.imread(\"image.jpg\")\n",
    "\n",
    "    dets = detector(frame[:, :, ::-1])\n",
    "    if len(dets) > 0:\n",
    "        parts = PREDICTOR(frame, dets[0]).parts()\n",
    "    for i in parts:\n",
    "        cv2.circle(frame, (i.x, i.y), 3, (255, 0, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"me2\", frame)\n",
    "    cv2.waitKey(0)    \n",
    "    capture.release()\n",
    "    \n",
    "    be_bean = BeBean('image.jpg')\n",
    "    be_bean.to_bean('image.jpg')\n",
    "    \n",
    "    out = cv2.imread(\"outputs/output_image.jpg\")\n",
    "    cv2.imshow(\"change3\", out)\n",
    "    cv2.waitKey(0)    \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
