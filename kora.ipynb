{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if cap.isOpened() is False:\n",
    "        raise(\"IO Error\")\n",
    "\n",
    "    cv2.namedWindow(\"Capture\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            continue\n",
    "\n",
    "        cv2.imshow(\"Capture\", image)\n",
    "       \n",
    "        if cv2.waitKey(33) >= 0:\n",
    "            cv2.imwrite(\"image.png\", image)\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    cv2.imshow(\"image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "Number of faces detected: 1\n",
      "msuo tvをロードしました.\n",
      "Number of faces detected: 1\n",
      "↓次回これやる　どのビーンズを使うかわかる↓解析してええええええええええええええ\n",
      "<__main__.Face object at 0x000001EE38D1A978>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "PREDICTOR_PATH = './shape_predictor_68_face_landmarks.dat'\n",
    "PREDICTOR = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "\n",
    "\n",
    "\n",
    "class NoFaces(Exception):\n",
    "    pass\n",
    "\n",
    "class Face:\n",
    "  def __init__(self, image, rect):\n",
    "    self.image = image\n",
    "    self.landmarks = numpy.matrix(\n",
    "      [[p.x, p.y] for p in PREDICTOR(image, rect).parts()]\n",
    "    )\n",
    "\n",
    "class BeBean:\n",
    "  SCALE_FACTOR = 1\n",
    "  FEATHER_AMOUNT = 11\n",
    "\n",
    "  # 特徴点のうちそれぞれの部位を表している配列のインデックス\n",
    "  FACE_POINTS = list(range(17, 68))\n",
    "  MOUTH_POINTS = list(range(48, 61))\n",
    "  RIGHT_BROW_POINTS = list(range(17, 22))\n",
    "  LEFT_BROW_POINTS = list(range(22, 27))\n",
    "  RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "  LEFT_EYE_POINTS = list(range(42, 48))\n",
    "  NOSE_POINTS = list(range(27, 35))\n",
    "  JAW_POINTS = list(range(0, 17))\n",
    "\n",
    "  ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS + RIGHT_BROW_POINTS +\n",
    "    NOSE_POINTS + MOUTH_POINTS)\n",
    "\n",
    "  # オーバーレイする特徴点\n",
    "  OVERLAY_POINTS = [LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
    "    NOSE_POINTS + MOUTH_POINTS]\n",
    "\n",
    "  COLOR_CORRECT_BLUR_FRAC = 0.7\n",
    "\n",
    "  def __init__(self, before_after = True):\n",
    "    self.detector = dlib.get_frontal_face_detector()\n",
    "    self._load_beans()\n",
    "    self.before_after = before_after\n",
    "\n",
    "  def load_faces_from_image(self, image_path):\n",
    "    \"\"\"\n",
    "      画像パスから画像オブジェクトとその画像から抽出した特徴点を読み込む。\n",
    "      ※ 画像内に顔が1つないし複数検出された場合も、返すので正確には「特徴点配列」の配列を返す\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (image.shape[1] * self.SCALE_FACTOR,\n",
    "                               image.shape[0] * self.SCALE_FACTOR))\n",
    "\n",
    "    rects = self.detector(image, 1)\n",
    "\n",
    "    if len(rects) == 0:\n",
    "      raise NoFaces\n",
    "    else:\n",
    "      print(\"Number of faces detected: {}\".format(len(rects)))\n",
    "\n",
    "    faces = [Face(image, rect) for rect in rects]\n",
    "    return image, faces\n",
    "\n",
    "  def transformation_from_points(self, t_points, o_points):\n",
    "    \"\"\"\n",
    "      特徴点から回転やスケールを調整する。\n",
    "      t_points: (target points) 対象の特徴点(入力画像)\n",
    "      o_points: (origin points) 合成元の特徴点(つまりビーン)\n",
    "    \"\"\"\n",
    "\n",
    "    t_points = t_points.astype(numpy.float64)\n",
    "    o_points = o_points.astype(numpy.float64)\n",
    "\n",
    "    t_mean = numpy.mean(t_points, axis = 0)\n",
    "    o_mean = numpy.mean(o_points, axis = 0)\n",
    "\n",
    "    t_points -= t_mean\n",
    "    o_points -= o_mean\n",
    "\n",
    "    t_std = numpy.std(t_points)\n",
    "    o_std = numpy.std(o_points)\n",
    "\n",
    "    t_points -= t_std\n",
    "    o_points -= o_std\n",
    "\n",
    "    # 行列を特異分解しているらしい\n",
    "    # https://qiita.com/kyoro1/items/4df11e933e737703d549\n",
    "    U, S, Vt = numpy.linalg.svd(t_points.T * o_points)\n",
    "    R = (U * Vt).T\n",
    "\n",
    "    return numpy.vstack(\n",
    "      [numpy.hstack((( o_std / t_std ) * R, o_mean.T - ( o_std / t_std ) * R * t_mean.T )),\n",
    "      numpy.matrix([ 0., 0., 1. ])]\n",
    "    )\n",
    "\n",
    "  def get_face_mask(self, face):\n",
    "    image = numpy.zeros(face.image.shape[:2], dtype = numpy.float64)\n",
    "    for group in self.OVERLAY_POINTS:\n",
    "      self._draw_convex_hull(image, face.landmarks[group], color = 1)\n",
    "\n",
    "    image = numpy.array([ image, image, image ]).transpose((1, 2, 0))\n",
    "    image = (cv2.GaussianBlur(image, (self.FEATHER_AMOUNT, self.FEATHER_AMOUNT), 0) > 0) * 1.0\n",
    "    image = cv2.GaussianBlur(image, (self.FEATHER_AMOUNT, self.FEATHER_AMOUNT), 0)\n",
    "\n",
    "    return image\n",
    "\n",
    "  def warp_image(self, image, M, dshape):\n",
    "    output_image = numpy.zeros(dshape, dtype = image.dtype)\n",
    "    cv2.warpAffine(\n",
    "      image,\n",
    "      M[:2],\n",
    "      (dshape[1], dshape[0]),\n",
    "      dst = output_image, borderMode = cv2.BORDER_TRANSPARENT, flags = cv2.WARP_INVERSE_MAP\n",
    "    )\n",
    "    return output_image\n",
    "\n",
    "  def correct_colors(self, t_image, o_image, t_landmarks):\n",
    "    \"\"\"\n",
    "      対象の画像に合わせて、色を補正する\n",
    "    \"\"\"\n",
    "    blur_amount = self.COLOR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n",
    "      numpy.mean(t_landmarks[self.LEFT_EYE_POINTS], axis = 0) -\n",
    "      numpy.mean(t_landmarks[self.RIGHT_EYE_POINTS], axis = 0)\n",
    "    )\n",
    "    blur_amount = int(blur_amount)\n",
    "\n",
    "    if blur_amount % 2 == 0: blur_amount += 1\n",
    "\n",
    "    t_blur = cv2.GaussianBlur(t_image, (blur_amount, blur_amount), 0)\n",
    "    o_blur = cv2.GaussianBlur(o_image, (blur_amount, blur_amount), 0)\n",
    "\n",
    "    # ゼロ除算を避ける　\n",
    "    o_blur += (128 * (o_blur <= 1.0)).astype(o_blur.dtype)\n",
    "\n",
    "    return (o_image.astype(numpy.float64) * t_blur.astype(numpy.float64) / o_blur.astype(numpy.float64))\n",
    "\n",
    "  def to_bean(self, image_path):\n",
    "    original, faces = self.load_faces_from_image(image_path)\n",
    "\n",
    "    # base_imageに合成していく\n",
    "    base_image = original.copy()\n",
    "\n",
    "    for face in faces:\n",
    "      bean = self._get_bean_similar_to(face)\n",
    "      bean_mask = self.get_face_mask(bean)\n",
    "\n",
    "      M = self.transformation_from_points(\n",
    "        face.landmarks[self.ALIGN_POINTS],\n",
    "        bean.landmarks[self.ALIGN_POINTS]\n",
    "      )\n",
    "\n",
    "      warped_bean_mask = self.warp_image(bean_mask, M, base_image.shape)\n",
    "      combined_mask = numpy.max(\n",
    "        [self.get_face_mask(face), warped_bean_mask], axis = 0\n",
    "      )\n",
    "\n",
    "      warped_image = self.warp_image(bean.image, M, base_image.shape)\n",
    "      warped_corrected_image = self.correct_colors(base_image, warped_image, face.landmarks)\n",
    "      base_image = base_image * (1.0 - combined_mask) + warped_corrected_image * combined_mask\n",
    "\n",
    "    path, ext = os.path.splitext( os.path.basename(image_path) )\n",
    "    cv2.imwrite('outputs/output_' + path + ext, base_image)\n",
    "\n",
    "    if self.before_after is True:\n",
    "      before_after = numpy.concatenate((original, base_image), axis = 1)\n",
    "      cv2.imwrite('before_after/' + path + ext, before_after)\n",
    "\n",
    "  def _draw_convex_hull(self, image, points, color):\n",
    "    \"指定したイメージの領域を塗りつぶす\"\n",
    "\n",
    "    points = cv2.convexHull(points)\n",
    "    cv2.fillConvexPoly(image, points, color = color)\n",
    "\n",
    "  def _load_beans(self):\n",
    "    \"Mr. ビーンの画像をロードして、顔(特徴点など)を検出しておく\"\n",
    "\n",
    "    self.beans = []\n",
    "    for image_path in glob.glob(os.path.join('Masuo TV', '*.jpg')):\n",
    "      image, bean_face = self.load_faces_from_image(image_path)\n",
    "      self.beans.append(bean_face[0])\n",
    "    print('msuo tvをロードしました.')\n",
    "\n",
    "  def _get_bean_similar_to(self, face):\n",
    "    \"特徴点の差分距離が小さいMr.ビーンを返す\"\n",
    "\n",
    "    get_distances = numpy.vectorize(lambda bean: numpy.linalg.norm(face.landmarks - bean.landmarks))\n",
    "\n",
    "    distances = get_distances(self.beans)\n",
    "    print(\"↓次回これやる　どのビーンズを使うかわかる↓解析してええええええええええええええ\")\n",
    "    print (self.beans[distances.argmin()])\n",
    "    return self.beans[distances.argmin()]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    if capture.isOpened() is False:\n",
    "        raise(\"IO Error\")\n",
    "\n",
    "    cv2.namedWindow(\"Capture\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ret, image = capture.read()\n",
    "\n",
    "        if ret == False:\n",
    "            continue\n",
    "\n",
    "        cv2.imshow(\"Capture\", image)\n",
    "       \n",
    "        if cv2.waitKey(33) >= 0:\n",
    "            cv2.imwrite(\"image.jpg\", image)\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    frame = cv2.imread(\"image.jpg\")\n",
    "\n",
    "    dets = detector(frame[:, :, ::-1])\n",
    "    if len(dets) > 0:\n",
    "        parts = PREDICTOR(frame, dets[0]).parts()\n",
    "    for i in parts:\n",
    "        cv2.circle(frame, (i.x, i.y), 3, (255, 0, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"me\", frame)\n",
    "    cv2.waitKey(0)    \n",
    "    capture.release()\n",
    "    \n",
    "    be_bean = BeBean()\n",
    "    be_bean.to_bean('image.jpg')\n",
    "    \n",
    "    out = cv2.imread(\"outputs/output_image.jpg\")\n",
    "    cv2.imshow(\"change\", out)\n",
    "    cv2.waitKey(0)    \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鼻 30\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 30\n",
      "鼻 31\n",
      "鼻 30\n",
      "鼻 31\n",
      "鼻 30\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 32\n",
      "鼻 30\n",
      "鼻 32\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 30\n",
      "鼻 31\n",
      "鼻 30\n",
      "鼻 30\n",
      "鼻 30\n",
      "鼻 30\n",
      "鼻 30\n",
      "鼻 30\n",
      "鼻 28\n",
      "鼻 30\n",
      "鼻 28\n",
      "鼻 28\n",
      "鼻 30\n",
      "鼻 28\n",
      "鼻 28\n",
      "鼻 31\n",
      "鼻 29\n",
      "鼻 30\n",
      "鼻 31\n",
      "鼻 31\n",
      "鼻 31\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # 確認用 ここまで ---\n",
    "    # ここに処理を追加していく ----\n",
    "    dets = detector(frame[:, :, ::-1])\n",
    "    if len(dets) > 0:\n",
    "      parts = predictor(frame, dets[0]).parts()\n",
    "\n",
    "    # 確認用 ---\n",
    "      img = frame * 0\n",
    "      for i in parts:\n",
    "        cv2.circle(img, (i.x, i.y), 3, (255, 0, 0), -1)\n",
    "\n",
    "      cv2.imshow(\"me\", img)\n",
    "\n",
    "      nose = parts[33].y - parts[27].y\n",
    "      print(\"鼻 %d\" %nose)\n",
    "    # ここまで ----\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鼻 30\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "# ここに処理を追加していく ----\n",
    "dets = detector(frame[:, :, ::-1])\n",
    "if len(dets) > 0:\n",
    "  parts = predictor(frame, dets[0]).parts()\n",
    "\n",
    "  # 確認用 ---\n",
    "  for i in parts:\n",
    "     cv2.circle(img, (i.x, i.y), 3, (255, 0, 0), -1)\n",
    "\n",
    "  cv2.imshow(\"me\", img)\n",
    "  # 確認用 ここまで ---\n",
    "\n",
    "  nose = parts[33].y - parts[27].y\n",
    "  print(\"鼻 %d\" %nose)\n",
    "\n",
    "\n",
    "  # ここまで ----\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
